- name: Precise Task Formalization Matters in Winograd Schema Evaluations
  link: 
  authors: Haokun Liu*, William Huang*, Dhara A. Mungra, Samuel R. Bowman
  pubtype: In Proceedings of the 2020 Conference on Empirical Methods in Natural Language-Processing (EMNLP 2020)
  description: "Performance on the Winograd Schema Challenge (WSC), a respected commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize part of this improvement comes from recent changes in task formalization by users of the dataset: the combination of input specification, loss function, and the way pretrained parameters are used. We perform an ablation on two Winograd Schema datasets that interpolates between the two formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii) several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model’s extreme sensitivity to hyperparameters. The impact of task formalization may result in overly optimistic reports of improved commonsense reasoning performance. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results."

- name: Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data
  link: 
  authors: William Huang, Haokun Liu, Samuel R. Bowman
  pubtype: In Proceedings of the EMNLP 2020 Insights from Negative Results Workshop
  description: "A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks---datasets collected from crowdworkers to create an evaluation task---while still failing on minimally perturbed examples. Recent work has explored the use of counterfactually-augmented data—data built by minimally editing a set of seed examples to yield counterfactual labels—to augment training data associated with these benchmarks and build more robust classifiers that generalize better. We use English natural language inference data to test model generalization and robustness and find (i) models trained on counterfactually-augmented SNLI data alone do not generalize better compared to similarly large unaugmented datasets and (ii) the data augmentations hurt performance on two diagnostic sets, yielding models that are less robust to adversarial examples and less competent along several lines of linguistic reasoning. We thus argue that careful consideration should be given to the trade-offs between seed examples and augmented data in counterfactuallyaugmented datasets and encourage researchers to explore this line of work before using such data for training."  